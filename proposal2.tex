\documentclass[submission]{eptcs}
\setlength{\parindent}{0pt}
\providecommand{\event}{} % Name of the event you are submitting to
\usepackage{listings}
\usepackage[utf8]{inputenc}

\lstdefinelanguage{Lambda}{%
  morekeywords={%
    if,then,else,fix,fun,let % keywords go here
  },%
  morekeywords={[2]int, bool},   % types go here
  otherkeywords={:}, % operators go here
  literate={% replace strings with symbols
    {->}{{$\to$}}{2}
    {lambda}{{$\lambda$}}{1}
  },
  basicstyle={\ttfamily},
  keywordstyle={\bfseries},
  keywordstyle={[2]\itshape}, % style for types
  keepspaces,
  mathescape % optional
}[keywords,comments,strings]%
\setlength{\parindent}{2em}   % choose your indent width

\title{Optimizing evidence memory representation on GrEv; an evidence based gradual typing compiler.}
\author{ 
\institute{Department of Computer Science\\
University of Chile\\
Santiago, Chile}
}
\begin{document}
\maketitle

\section{Introduction}

% background: existencia de grev, que quiero poder evaluar de mejor manera y optimizar

In the world of software engineering we often find ourselves in the dillema of choosing between two distinct tools: a flexible dynamic typed language, or a reliable statically typed one.
On one side, dynamically typed languages like Python and JavaScript prioritize developer flexibility, rapid prototyping and expressiveness by deferring type checking to runtime.
On the other side, statically typed languages like OCaml and Rust prioritize early error detection, correctness and performance through ahead-of-time type verification.
Gradual typing has emerged as the principal option to reconcile this opposing paradigms, allowing a single language to support both disciplines seamlessly.
By enabling developers to control the precision of static type checking at a fine-grained level (down to individual function arguments or variable bindings) gradual typing promises to 
deliver the best of both worlds, facilitating the evolution of scripts into robust applications without necessitating a complete rewrite in a different language.

However, the practical realization of gradual typing comes with significant engineering challenges.
The core difficulty lies in enforcing soundness without having considerable performance overheads.

% However, the realization of this promise is fundamentally constrained by the "performance cliff".
% Sound gradual typing requires that the runtime system enforce the invariants assumed by statically typed code components when they interact with dynamically typed (or less precisely typed) components.
% If this checks are inefficient, the overhead of mixing types can result in performance degradation so severe that it renders the gradual language impractical for production use.
% Consequently, the central research challenge in this field is the development of compilation techniques and runtime memory representations that minimize this interoperability overhead.

To date, the Grift compiler stands as the reference implementation for high-performance, close-to-the-metal gradual typing.
Grift employs a coercion-based approach, where type consistency checks are reified as "coercions" (specialized data structures representing the path between source and target types).

An alternative, and theoretically distinct, approach is Abstracting Gradual Typing (AGT).
AGT re-imagines gradual typing not as a translation to a cast calculus, but through the lens of Abstract Interpretation.
This model uses evidence; a runtime value that serves as justification for plausibility of a consistent typing judgment, to check for types.

The GrEv compiler represents the first attempt to build a high-performance compiler based on evidence.
The compiler has different semantic modes that change the way the language interacts with evidence.
This modes are GrEv/G (proxy, supports "normal" gradual typing semantics), GrEv/MC (monotonic closures) and GrEv/MV (monotonic values).
Preliminary evaluations indicate that GrEv is not only competitive with Grift but outperforms it in specific scenarios.
Also, results show that GrEv is more stable across configurations on the static-to-dynamic spectrum.
Despite its great results, there still is a lot of room for improvement, and one of the biggest overheads that GrEv has is its evidence memory representation.

This thesis proposal aims to advance the state of the art of evidence-based gradual typing by exploring and implementing optimizations on memory representation of evidence on GrEv.

\section{Related Work}

To understand the necessity of the proposed research, one must first analyze the mechanisms employed by the current state-of-the-art compilers, Grift and GrEv.

\subsection{Grift: Coercions and Space Efficiency}

Grift serves as the reference implementation for coercion-based gradual typing. 
In this model, the static semantics of the gradual language are elaborated into an intermediate cast calculusm and this casts are then compiled into coercions.
One of the main challenges in this systems is that coercions can be accumulated.
For example, consider a function that is passed from a typed module to an untyped module and back again multiple times.
In a naive implementation, each boundary crossing adds a new wrapper to the function, wasting a lot of memory.
Grift mitigates this problem via coercion composition.
Whenever a new coercion is applied to an already coerted value, the runtime system attempts to compose the new coercion with the existing one.
If the coercions are inverses (casting \textit{Int} to \textit{?} and then back to \textit{Int}, for example), they cancel each other out, and the wrapper is removed.
If they are compatible, they are merged into a canonical form.
This normalization process ensures that the space overhead of wrappers remains bounded.

Grift also presents an optimization on access to mutable state using monotonic references.
In a traditional "guarded" approach, a mutable reference is wrapped in a proxy whenever it crosses a type boundary.
This allows different parts of the program to have different, potentially incompatible views of the same heap cell.
However, this requires every read and write operation to go through the proxy to get to the value.

Grift's monotonic mode (GriftMS) makes it so that every time a reference is cast to a more precise type, the original heap cell is updated.
If the cell contains a structure, it is recursively traversed to ensure all reachable values conform to the new type.

\subsection{GrEv: Evidence-based Compilation}

GrEv represents a paradigm shift, implementing the runtime semantics of AGT directly.
Instead of translating types into coercions, GrEv preserves the structure of gradual types at runtime in the form of evidence.
To better understand what evidence is, we first need to introduce some concepts:

% \begin{figure}[t]
%   \begin{displaymath}
%     \begin{array}{r@{\hspace{0.3em}}c@{\hspace{0.8em}}l@{\hspace{0.8em}}l}
%       e & ::= & \fun{\overline{x: \cT}}{e} \mid e~e\ldots \mid x  & \text{(functions and variables)} \\
%       & &  \mid n \mid f \mid \binop{e}{e} \mid \mathit{op}~e\ldots & \text{(integers, floats, and operators)} \\
%       & &  \mid b \mid \ifthenelse{e}{e}{e}  & \text{(booleans and conditionals)} \\
%       & &  \mid \loopit{x}{e}{e}{e} & \text{(loops)}\\
%       & &  \mid\letin{x: \cT}{e}{e} \mid \letrecin{f: \cT}{e}{e} & \text{(let bindings)} \\
%       & &  \mid\boxit{e} \mid \unbox{e} \mid \assign{e}{e} & \text{(references, unit)}\\
%       &  &   \mid\vector{e}{e} \mid \vectorset{e}{e}{e} \mid \vectorget{e}{e}& \text{(vectors)} \\
%       &  &  \mid\tuple{\overline{e}} \mid \proj{e}{e} & \text{(tuples)} \\
%       & &  \mid \datastruct{N} \mid C~e\ldots  & \text{(variant definition and construction)} \\
%       & &  \mid \match{e} & \text{(variant elimination)} \\
%       & & \mid e :: \cT \mid e ; e& \text{(ascriptions and sequences)} \\
%       \cT & ::= & \tint \mid \tbool \mid \tfloat \mid \tref{\cT} \mid \tvec{\cT} 
%       & \text{(types)} \\ 
%       %\ttuple{\overline{\cT}}
%       & & \mid \tunit \mid \cT * \cT \ldots \mid \tfun{(\overline{\cT})}{\cT} \mid N \mid \tunk  &  
%     \end{array}
%   \end{displaymath}
%   \caption{\lang source syntax}
%   % \mt{what is the second argument of vector? Are recursive types missing?}
%   %\et{box/unbox/setbox or ml-style !, :=, ref}\et{finalize keywords and update lstlisting}
%   % \et{fix syntax operators, list them}\et{what about unit?}}
%   \label{fig:lang-syntax}
% \end{figure} 

\subsubsection{Concretization and Abstraction}

The key idea in the AGT framework is to view gradual types as abtractions of sets of static types, formally defined by a Galois connection.
This connections define two functions, the concretization function $\gamma$ and the abstraction function $\alpha$, both functioning as a conection between sets which, in this case, are GTYPE (the set of all gradual types), and $P(TYPE)$ (the set of all sets that can be created from static types).

The concretization function ($\gamma$) maps a gradual type to the set of static types it represents.
For precise types (like int or bool), the mapping is singular:

$$\gamma(int) = {int}$$

and, as we said before, the unknown type $?$ concretizes to the set of all possible static types in the language:

$$\gamma(?) = TYPE$$

Structural types propagate this relation. A gradual function type $G_1 \rightarrow G_2$ concretizes to the set of all static function types $T_1 \rightarrow T_2$ where $T_1$ is the concretization of $G_1$ and $T_2$ is the concretization of $G_2$.

With this definition we can think about \textbf{precision}.
A gradual type $G_1$ is less precise than $G_2$ if the set of static types represented by $G_1$ (i.e. its concretization) is a superset of those represented by $G_2$.

The abstraction function ($\alpha$) maps a set of static types to the more precise gradual type that represents the set.
For example:

$$\alpha(\{int, bool\}) = ?$$

or

$$\alpha(\{int \rightarrow bool, int \rightarrow string\}) = int \rightarrow ?$$

\subsubsection{Consistency}

In a static system there could be different type relations, like equality, subtyping, containtment, etc.
What AGT proposes is that, in order to get the statics semantics of a gradual system, this relations must be lifted into their gradual counterparts.

In a general manner, to lifting of a relation $R$ is denoted by:

$$\tilde{R}(G_1, G_2) = \exists T_1 \in \gamma(G_1), T_2 \in \gamma(G_2), R(T_1, T_2)$$

which in simple words means that $\tilde{R}$ of two gradual types $G_1$ and $G_2$ is valid if there exist two static types $T_1$ and $T_2$ in the sets that each gradual type represents where $R(T_1, T_2)$ is valid.

In the case of GrEv, the only relation defined is equality, and its gradual counterpart is called consistency (denoted by $\sim$).
We say that two gradual types $G_1$ and $G_2$ are consistent if there exist a type $T$ that belong to the concretization of both $G_1$ and $G_2$ (or in other words, if there exist two static types in each concretization that fulfill the relation i.e. are equal).

$$G_1 \sim G_2 \Leftrightarrow \gamma (G_1) \cap \gamma (G_2) \ne \emptyset$$

This explains why $int$ is consistent with $?$ ($int$ is in the set of all types), but not consistent with $bool$.
A very important detail is that, different from equality, \textbf{consistency is not transitive}.
If it were, we could have something like the following:

$$int\sim ? \land ? \sim bool \Rightarrow int \sim bool$$

which breaks everything.

\subsubsection{Evidence and Consistent Transitivity}

AGT provides a direct dynamic semantics of gradual programs by applying proof reduction on gradual typing derivations.

While in a fully static language equality between types is transitive, in a gradual setting we can not statically make sure that the transitivity of consistency between types is valid.
Lets introduce the following program:

\begin{lstlisting}[language=Lambda]
let f: int -> int = fun x -> x + x in 
  f (true :: ?)
\end{lstlisting}

In this example, the function \textbf{\textit{f}} expects an \textit{int} and a value of type \textit{?} is given.
This program type checks because $? \sim int$, but in runtime, as the argument of the function given is a \textit{bool}, we have a chain:

$$bool \sim ? \land ? \sim int$$

So dynamically something must be done to justify this judgement.

Evidence is a runtime object that carries information about the type of some judgement, and has its own types $ETYPE$.

To illustrate a more general case, lets introduce a gradual language where static types are natural numbers, and the only relation between them is the "less than" comparison ($\leq$)

Imagine now that we have the following judgement:

$$ 5 \tilde\leq ? $$

In order to justify this judgement, we must provide evidence that show us why this is valid.
Using the definition of gradual types that represent a set of static types, this judgement is valid because $\gamma(?)$ contains various numbers that are greater or equal than 1 (in other words, there exist at least one type that suffies the lifting of the $\tilde\leq$ relation)
Actually, we could use $ETYPE = GTYPE$ to justify this judgement, because there exist a type in $GTYPE$ whose concretization contains all the values that suffies the relation, that is, $\gamma(?)$, but this is not precise enough.
That's why it's prefered in this case to use intervals as $ETYPE$, so in the case of the previos judgement, we can use the interval $[5, \infty]$.

Evidence for this judgement would look like this:
$$\langle[5, 5], [5, \infty]\rangle \vdash 5 \tilde\leq ?$$

Note that, for the type \textit{5} we use the interval $[5, 5]$, because evidence must be expressed with $ETYPE$

We define the interior of a judgement between two gradual types $G_1$ and $G_2$ with the relation $R$ ($I_R(G_1, G_2)$) like the following:

$$I_R(G_1, G_2) = \alpha^{2}_E(\{\langle T_1, T_2 \rangle \mid T_1 \in \gamma(G_1), T_2 \in \gamma(G_2), R(T_1, T_2)\})$$

This is the initial evidence of a judgement, and can be understood in two steps:
\begin{enumerate}
  \item First we calculate the two set of static type pairs $T_1$ and $T_2$ where the relation $R$ is valid.
  \item Then, we calculate the abstraction from the TYPE set into the ETYPE set ($\alpha^{2}_E$)\footnote{$\alpha^2(\{\langle T_{i1}, T_{i2} \rangle, ...\}) = \langle \alpha(\{T_{i1}, ...\}), \alpha(\{T_{i2}, ...\}) \rangle$} to get the evidence type $E$ that better represents this two TYPE sets.
\end{enumerate}

Note that we are using an abstraction function between TYPE and ETYPE.
In order to do this, a Galois connection between TYPE and ETYPE must be defined, including the abstraction and concretization functions.
This functions follow the same definition given in the previously.

Now we would like to define how to check whether the relation $R$ applies to two gradual types $G_1$ and $G_3$ applies, given that the relation applies between $G_1$ and $G_2$, and also between $G_2$ and $G_3$: 

$$\epsilon_1 \vdash \tilde{R}(G_1, G_2) \land \epsilon_2 \vdash \tilde{R}(G_2, G_3) \Rightarrow \tilde{R}(G_1, G_2)$$

For this, we need to check inside the evidences $\epsilon_1$ and $epsilon_2$, and see if there exists a subset of TYPEs in the intersection between the concretization of each part of the innermost part of this evidences, where the relation $R$ is valid.
This operation is called \textit{Consistent Transitivity}, its denoted by $\circ^{R}$ and serves the functionality of checking whether a relation between two gradual types is valid evaluating transitivity.

$$\epsilon_1 \circ^{R} \epsilon_2 = \langle E_1, E_2 \rangle \circ^{R} \langle E_1, E_2 \rangle = \alpha(\{\langle T_1, T_4 \rangle \mid T_1 \in \gamma_E(E_1), T_4 \in \gamma_E(E_4), T_{2\cap 3} \in \gamma_E(E_2) \cap \gamma_E(E_3), R(T_1, T_{2\cap 3}) \land R(T_{2\cap 3}, T_4)\})$$

This operation can be understood in N steps:

\begin{enumerate}
  \item First, we calculate the concretization $\gamma_E$ of every evidence type $E_i$ to get the set of static types $T_i$ that are represented by $E_i$.
  \item Then, to justify transitivity we have to check if there exist a pair of TYPEs in the intersection of the innermost parts of the evidences we are checking\footnote{if $\langle E_1, E_2 \rangle \circ^{R} \langle E_1, E_2 \rangle$, the inner part would be $E_2$ and $E_3$} that suffies the relation $R$.
  \item If such pair exists, then the transitivity is justified and the evidence is updated with the outtermost parts of the evidences.
  \item If such pair does not exist, then the transitivity is not justified, and the dynamic system should throw an error.
\end{enumerate}

Lets ilustrate this two operations with an example.
We want to check consistent transitivity between the two following judgements:

\begin{math}
  10\tilde\leq ? \\
  ?\tilde\leq 20
\end{math}

In order to check if $10 \tilde\leq 20$, we can use the $I_{\tilde\leq}$ and $\circ^{\tilde\leq}$.
First, for notation sake, a set of numbers from $a$ to $b$ will be represented as $S_{[a, b]}$.
Now, to calculate the interior, we calculate first the concretization of the gradual types:

\begin{math}
  \gamma(10) = \{10\} \\
  \gamma(20) = \{20\} \\
  \gamma(?) = Nat
\end{math}


With these, we can calculate the interior of the first judgement to get its initial evidence:

\begin{math}
  I_{\tilde\leq}(10, ?) = \alpha^{2}_{E}(\{\langle T_1, T_2\rangle \mid T_1 \in \{10\}, T_2 \in Nat, T_1 \leq T_2\}) \\
  = \langle \alpha(\{10\}), \alpha(S_{[10, \infty]}) \rangle = \langle [10, 10], [10, \infty] \rangle
\end{math}

The same way, we can calculate the interior of the second judgement and get the following:

\begin{math}
  \langle [10, 10], [10, \infty] \rangle \vdash 10\tilde\leq ? \\
  \langle [1, 20], [20, 20] \rangle \vdash ?\tilde\leq 20
\end{math}

Now, in order to check if the judgement, we have to calculate the consistent transitivity over the two evidences.
The concretization of the evidences $\gamma_E$ is

\begin{math}
  \gamma_E([10, 10]) = \{10\} \\
  \gamma_E([10, \infty]) = S_{[10, \infty]} \\
  \gamma_E([1, 20]) = \{1, 2, ..., 20\} \\
  \gamma_E([20, 20]) = \{20\}
\end{math}

and with that, the consistent transitivity of the evidences is:

\begin{math}
  \langle [10, 10], [10, \infty] \rangle \circ^{\tilde\leq} \langle [1, 20], [20, 20] \rangle \\
  = \alpha(\{\langle T_1, T_4\rangle \mid T_1 = 10, T_4 = 20, T_{2\cap 3} \in \{10, 11, ..., 20\}, 10 \leq T_{2\cap 3} \land T_{2\cap 3} \leq 20\}) \\
  \alpha(\langle 10, 20 \rangle) = \langle [10, 10], [20, 20] \rangle
\end{math}

Finally, because we could calculate the consistent transitivity, the judgement $10 \tilde\leq 20$ is valid:

$$\langle [10, 10], [20, 20] \rangle \vdash 10 \tilde\leq 20$$



When more complex types and relations are available (like subtyping, for example), evidence needs to be able to store the information about the types involved and the relations between them.
GrEv only has simple types, so evidence can be represented as a single type.





% We have a function \textit{double} with type $?$ that expects its parameter to be an $int$ (we know that because its definition has the $+$ operator, that is only valid with $int$), but we call it with the value \textit{true} of type $bool$.
% We know statically that $bool \sim ?$ and $? \sim int$, but in this case the transitivity is not valid because \textit{bool} is not consistent with \textit{int}.
%
% To solve this problem, AGT proposes the use of \textit{evidence} to make sure that the transitivity between this consistenty judgements holds.
% Evidence is a runtime object that justifies a specific consistency judgement. 
% As a value flows from a context to another (like in the example), evidence is checked to see if the types are compatible, and combined into a more precise state.
% This operation is called \textit{Consistent Transitivity}.

\subsubsection{GrEv's evidence implementation}

% Apart from GrEv's gradual modes, GrEv also has it's static and dynamic versions (StaticGrEv and DynGrEv, respectively).
% This versions follow the basic principles of compilation for statically and dynamically typed languages.
As mentioned before, the current implementation of GrEv only supports simple types, so evidence can be represented as a simple type.
GrEv differentiate between inmmediate and boxed values.
Boxed values have an specific field in memory that saves its evidence.
For example, a reference is stored as two words (16 bytes), one as the evidence and the other as the value.
Inmmediate values are tagged, so there is no need to have extra memory to get it's evidence.

\subsection{GrEv modes}

As stated in the introduction, GrEv supports different semantic modes that interacts with evidence in different ways.
To ilustrate the semantic each of the modes supports, let's introduce the following program:

\begin{lstlisting}[language=Lambda]
let double: ? = fun x -> x + x in 
  foo 1;
  foo 1.0
\end{lstlisting}

\subsubsection{GrEv/MC: Monotonic closures}

GrEv/MC implements monotonic closures, this is, the evidence for a closure is global.
This means that, when the evidence of the closure is updated, this affects every other application of the function.
In the example, when \textit{foo} is applied to 1, its evidence is updated from \textit{?} to $int \rightarrow int$, thus its following application throws an error.


\subsubsection{GrEv/G: Guarded}

GrEv/G uses proxys to store values in the heap separated from their evidence.
In this version, every time  the value of an evidence is updated, a shallow copy is created with a new memory cell that contains this update.
In this case, after the first application of \textit{foo}, a new memory cell is created that contains the evidence $int \rightarrow int$, so the second application throws no error.

\subsubsection{GrEv/MV: Monotonic values}

GrEv/MV uses monotonic evidences for every value in the language.
In this sense, when the evidence of a reference, a vector, a closure, etc. is updated, it is updated globally and affects the following uses of that value.

\section{Problem}

In order to initialize and update the different evidences in a program, the following operations must be implemented:
\begin{itemize}
  \item Interior
  \item Meet
  \item Cod
  \item Dom
\end{itemize}

The current representation of evidence is not optimal time and spacewise, specially for data structures and functions.

\section{Research questions}

The research questions regarding this thesis are the following:

\begin{itemize}
  \item Do the proposed optimization on evidence memory representation help to reduce the overhead of the current implementation of GrEv?
  \item How much does the current implementation of evidence in GrEv's differents gradual modes affect on its overhead over StaticGrEv?
  \item How much can an optimization on GrEv's Evidence memory representation improve its performance?
\end{itemize}

\section{Hypothesis}

The current implementation of GrEv's evidence affects significantly on its overhead over StaticGrEv, and its possible to reduce it by using a more clever implementation that is optimized for the operations needed.

\section{Main Objective}

The main objective of this proposal is to optimize GrEv's evidence current memory representation using a more suitable data structure, that is more efficient on evidence specific operations: meet, cod, dom, interior and equality.

\section{Specific Goals}

The specific goals of this research proposal are the following:

\begin{itemize}
  \item Find data structures that better suit the need of GrEv's evidence.
  \item Implement a standarized benchmark suite (like GTP) to get better insight on GrEv's current state and the different evidence representations performances.
  \item Measure different implementations to get better insight on what operations have more overhead in specific cases.
\end{itemize}

\section{Methodology}

To acomplish such goals, the following steps will be followed:

\subsection{Initial Exploration}

\begin{itemize}
  \item \textbf{State of the art in-depth review.} An in-depth review of the state of the art on gradual typing implementations will be conducted. The main goal of this stage is to clarify concepts and understand optimizations made in other implementations.
  \item \textbf{}
\end{itemize}

\subsection{GrEv's Modularization and Expansion}

\begin{itemize}
  \item \textbf{Benchmark Analisis.} The GTP benchmarks suite \cite{GTP} consists on 21 benchmarks, each containing and measuring different characteristics of the language, some of which are not currently implemented currently on GrEv. There will be a selection of the most important benchmarks from this suite for the current version of GrEv, and also an analisis of which features will be implemented depending on the relevance of the benchmarks that GrEv currently could not support.
  \item \textbf{Language Modularization.} The current state of GrEv's code does not support modular Evidence memory representations. There will be an analisis on what the best way to modularize GrEv's evidence implementations, so that it's easier to add and change between different versions.
  \item \textbf{Language Extension.} Depeding on the selected benchmarks, the necessary features will be implemented on GrEv.
  \item \textbf{Benchmark Porting.} There will be a manual port of the selected benchmarks from Racket (GTP's source language) to GTLC+.
\end{itemize}

\subsection{Experimental Evaluation}

\begin{itemize}
  \item \textbf{Optimization Evaluations.} After implementing each optimization, there will be experimental evaluation to check how this affect on GrEv's performance. The optimized versions will be compared between themselves, with Grift (the state-of-the-art gradual typing compiler) and GrEv's different configurations, depending on the context. This comparisons will consider time and space performances.
\end{itemize}

\section{Contributions}

\section{Conclusion}

% \section{Main Goal}
%
%
% \section{Specific goals}
%
% \begin{itemize}
% \item goal 1
% \end{itemize}
%
% \section{Methodology}
%
% texto
%
% \section{Expected results}
%
%  y contribuciones

\nocite{*}
\bibliographystyle{eptcs}
\bibliography{bibliography}
\end{document}
